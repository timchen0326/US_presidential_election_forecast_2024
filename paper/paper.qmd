---
title: "Forecasting the 2024 U.S. Presidential Election"
subtitle: "Modeling State-Level Polling Data to Forecast the Electoral College Outcome"
author: 
  - Tim Chen
  - Steven Li
  - Tommy Fu
thanks: "Code and data are available at: https://github.com/timchen0326/US_presidential_election_forecast_2024.git."
date: today
date-format: long
abstract: "We forecast the outcome of the 2024 U.S. Presidential Election between Kamala Harris and Donald Trump by developing multiple linear regression models based on comprehensive polling data collected throughout the election cycle. Incorporating variables such as state-level demographics, pollster reliability scores, transparency scores, and sample sizes, we applied the same statistical model to both candidates to ensure consistent comparison. Our analysis predicts that Kamala Harris will receive 216 electoral votes, while Donald Trump is projected to secure 147 electoral votes. Neither candidate achieves the 270 electoral votes required to win the presidency, highlighting the potential for a closely contested election. These findings underscore the significant impact of state-specific factors on voter support and suggest that neither candidate currently holds a decisive advantage. We recommend that future research includes dynamic modeling techniques and additional predictive variables, such as economic indicators and voter turnout rates, to enhance the accuracy of election forecasts. Our study emphasizes the complexities involved in electoral predictions and the necessity of balancing multiple factors in policy design and electoral analysis."
format: pdf
number-sections: true
toc: true
bibliography: references.bib
editor: 
  markdown: 
    wrap: sentence
---

```{r}
#| include: false
#| warning: false
#| message: false
# Set a CRAN mirror
# Set a specific CRAN mirror (this example uses the US CRAN mirror)

options(repos = c(CRAN = "https://cran.rstudio.com/"))

library(tidyverse)
library(ggplot2)
library(janitor)
library(modelsummary)
library(here)
library(dplyr)
library(knitr)
library(car)
library(maps)
library(stringr)
library(reshape2)
library(patchwork)


# Load the dataset
data <- read.csv(here::here("data/02-analysis_data/analysis_data.csv"))

```

\newpage
# Introduction

The 2024 United States presidential election presents unprecedented challenges for electoral forecasting. As the country navigates increasing political polarization and evolving voting patterns, the reliability of traditional polling methods has come under intense scrutiny [@viala2024]. The task of predicting voter behavior in America's diverse electorate is complicated by numerous factors, including shifting public opinion, rapidly changing political landscapes, and varying levels of voter engagement across different demographic groups.

Recent history has highlighted the complexities of election forecasting. The polling failures in 2016 and 2020—where polls significantly underestimated Republican support in key states—have prompted a fundamental reassessment of polling methodologies [@keeter2024]. These challenges are particularly acute in swing states, where margins of victory are often razor-thin and can determine the outcome of the entire election. The American Association for Public Opinion Research (AAPOR) identified several critical factors contributing to these polling errors, including the underrepresentation of Republican voters and difficulties in predicting voter turnout patterns [@viala2024].

Survey methodology plays a crucial role in addressing these challenges. Well-designed surveys require careful consideration of sampling strategies, questionnaire design, and data collection methods to ensure representative results. As Keeter [-@keeter2024] emphasizes, pollsters must now employ sophisticated weighting procedures and rigorous quality controls to overcome declining response rates and potential partisan non-response bias. Understanding the strengths and limitations of different polling approaches—from traditional probability sampling to newer online panels—is essential for accurate electoral forecasting.

This paper develops statistical models to forecast the outcome of the 2024 presidential election between Kamala Harris and Donald Trump. By leveraging multi-linear regression models, we predict the percentage of support for each candidate across different states, incorporating key variables such as pollster rating, sample size, and state-level demographics. Through aggregating these state-level predictions, we simulate Electoral College outcomes to provide insights into each candidate's probability of securing the required 270 electoral votes. Our analysis also includes a detailed examination of YouGov's polling methodology and proposes an idealized survey approach that could enhance the accuracy of election forecasting.

The remainder of this paper is structured as follows. @sec-data discusses the data used for this analysis, including key variables and sources, with particular attention to the quality metrics that affect polling accuracy. @sec-model outlines our modeling approach for each candidate, incorporating lessons learned from recent electoral cycles. @sec-predict presents our Electoral College predictions based on the model outputs. @sec-discuss discusses the implications of our findings and suggests directions for future research. Finally, @sec-appendix evaluates YouGov's polling methodology, and our idealized survey methodology.


# Data {#sec-data}
## Overview
Our study utilizes polling data from FiveThirtyEight's 2024 Presidential Election Forecast Database [@FiveThirtyEightPollingData2024], a comprehensive polling dataset maintained by ABC news. This database aggregates and standardizes polling results from various organizations, applying quality metrics and methodological assessments to each poll. In this section, we detail our selected variables, discuss key measurements, outline important limitations of our data, and our data cleaning process.

Our analysis focuses on several key variables that directly influence polling accuracy and electoral predictions. @tbl-overview_data presents a sample of our dataset.

## Poll Quality Variables

@tbl-poll-metrics summarizes three primary metrics that assess poll quality in our dataset:

- **Pollscore**: Represents "the error and bias attributable to a pollster, where negative numbers indicate better performance." Our data shows a mean pollscore of -1.06 (SD = 0.28), with values ranging from -1.50 to -0.50, indicating generally high-quality polling organizations in our sample.
- **Numeric Grade**: A rating given to pollsters to indicate their quality and reliability. With a mean of 2.89 (SD = 0.10) and a range of 2.70 to 3.00, our dataset maintains high standards. We established a minimum threshold of 2.7 to ensure methodological rigor.
- **Transparency Score**: A measure reflecting pollsters' transparency about their methodology, with 10 being the highest possible score. The data shows strong overall transparency with a mean of 8.59 (SD = 1.04), indicating that most polls in our sample maintain high standards of methodological disclosure.

## Methodological Variables

- **Methodology**: @fig-methodology-dist shows the distribution of polling approaches used to conduct the polls. Live Phone methods and Online Panels are the predominant methods, with hybrid approaches combining multiple methods (such as IVR/Online Panel/Text-to-Web) representing evolving polling techniques adapting to changing communication patterns.
- **Sample Size**: @fig-sample-size illustrates the distribution of poll sample sizes, representing the total number of respondents participating in each poll. Our dataset shows considerable variation, with a mean of 1,114.76 respondents (SD = 583.72). The right-skewed distribution ranges from 450 to 4,253 respondents, with larger sample sizes typically associated with national polls.

## Geographic and Temporal Variables

- **State Coverage**: @tbl-state-polls reveals the distribution of polls across different U.S. states. National polls lead with 261 surveys, followed by concentrated polling in key battleground states: Pennsylvania (86), Wisconsin (82), and North Carolina (64). This distribution reflects strategic focus on states where the electoral outcome is less certain.
- **End Date**: @fig-temporal-polling tracks polling frequency from July through October 2024. We observe increased activity during September and early October, with weekly poll counts peaking in late September. This pattern reflects intensified polling efforts as the election approaches.

## Outcome Variable

- **Support Percentage**: @fig-candidate-support displays our primary dependent variable: the percentage of respondents expressing support for each candidate. The distribution reveals a competitive race between Donald Trump and Kamala Harris, with support levels typically ranging between 40-50% for both candidates.


## Measurement and Limitations
There are several measurement and limitation considerations for our dataset:

- **Poll Quality:** While our pollscore and numeric grade filters help ensure data quality, these metrics are based on historical performance and may not fully capture current methodological improvements or deterioration.
- **Temporal Dynamics:** Our dataset provides discrete snapshots of voter preferences rather than continuous measurement. This limitation is particularly relevant given the rapid evolution of political narratives and voter sentiment during presidential campaigns.
- **Geographic Coverage:** Although we have national and state-level polling data, coverage varies by state. Battleground states typically have more frequent polling, while safer states may have sparse data, potentially affecting our state-level predictions.
- **Response Bias:** Despite careful methodology by pollsters, self-selection bias in survey participation and social desirability bias in responses remain potential concerns.

## Cleaning Process and Analysis

The data cleaning process employed R [@citeR] along with several specialized packages: tidyverse [@tidyverse] for data manipulation, dplyr [@dplyr] for data transformation, janitor [@janitor] for consistent naming conventions, and lubridate [@lubridate] for date handling. 

Our cleaning process followed several key steps:

1. Filtered for high-quality polls using a minimum threshold (*numeric_grade* $≥$ 2.7)
2. Limited temporal coverage to post-campaign announcement period (after July 21, 2024)
3. Standardized geographic data:

  > i. Coded missing state information as "National" polls
  > ii. Verified state names for consistency
  
4. Transformed key variables:

  > i. Converted *End Date* values to standardized date format
  > ii. Calculated absolute supporter numbers from percentages and sample sizes
  > iii. Created binary candidate indicators (Harris = 1, Trump = 0)

Other variable such as ... were not selected for our analysis because ...

Our variable selection process prioritized measures essential for polling accuracy and electoral prediction while eliminating redundant or non-informative fields. We retained key poll quality metrics including *pollscore* and *numeric_grade*, which provide crucial information about the reliability of each survey. Sample characteristics such as *sample size* and *methodology* were preserved to account for differences in polling precision. *State* and polling *end_date* information were maintained to capture regional variations and time-dependent patterns in voter preferences. 

We excluded several variables that offered little additional analytical value, such as *population_full*, which duplicated information available in other fields, and administrative data that did not directly influence predictions. This focused approach to variable selection allowed us to preserve all information necessary for accurate electoral forecasting.

# Modeling Support for the Candidates {#sec-model}

To analyze factors influencing candidate support, we use a linear regression model with sample size as the primary predictor. This model examines the relationship between sample size and the percentage of support for Kamala Harris and Donald Trump across different polls. By including sample size as a predictor, we can assess whether larger or smaller sample sizes impact reported support levels for each candidate.

```{r}
#| echo: false
#| message: false
#| warning: false
# Load necessary library

# Filter the data for only "Kamala Harris"
harris_data <- subset(data, candidate_name == "Kamala Harris")

# Perform a simple linear regression with pct as the dependent variable and sample_size as the independent variable
model <- lm(pct ~ sample_size, data = harris_data)
```

For Kamala Harris, the linear regression results show that sample size has a statistically insignificant effect on her support percentage. This implies that other factors, such as pollster methodology or regional biases, may play a more substantial role in shaping her reported support.

In contrast, the results for Donald Trump indicate a weak but statistically significant negative effect of sample size on his support percentage. This suggests that, on average, larger sample sizes tend to show slightly lower support for Trump, though the effect size remains small.

Figures @fig-harris-samplesize-supportpct and @fig-trump-samplesize-supportpct illustrate the relationship between sample size and support percentage for Kamala Harris and Donald Trump, respectively. These plots provide a visual representation of the trends observed in the linear regression models for each candidate.


```{r}
#| echo: false
#| message: false
#| warning: false
# Load necessary library

# Filter the data for only "Kamala Harris"
trump_data <- subset(data, candidate_name == "Donald Trump")

# Perform a simple linear regression with pct as the dependent variable and sample_size as the independent variable
model <- lm(pct ~ sample_size, data = trump_data)


```

```{r}
#| echo: false
#| message: false
#| warning: false

mlr_harris_model <- readRDS(here::here(("models/mlr_harris_model.rds")))
mlr_trump_model <- readRDS(here::here(("models/mlr_trump_model.rds")))
```

## Multiple Linear Regression Models

To better capture the complexity of voter support, we constructed multiple linear regression (MLR) models for both Kamala Harris and Donald Trump. These models incorporate key predictors, including pollster rating (represented by pollscore), transparency score, sample size, and state-level data. By including these variables, the MLR models aim to control for multiple factors influencing voter support and to enhance the accuracy of our predictions.

The MLR model for Kamala Harris indicates that factors such as state and pollster rating significantly impact her predicted level of support across polls. The results, shown in @fig-mlr-harris, suggest that Harris's support varies substantially by state and depends on the credibility of the pollster. Diagnostic checks, including residual and QQ plots, confirm that the model reasonably meets linear regression assumptions.

Similarly, the MLR model for Donald Trump highlights that state-level differences and pollster transparency play a significant role in explaining his support. Figure @fig-mlr-trump illustrates these findings, and diagnostic evaluations confirm that the model assumptions (linearity, independence, and homoscedasticity) are sufficiently met.


## Multicollinearity Check Using Variance Inflation Factor (VIF)

To ensure that the predictors used in both models do not exhibit multi-collinearity, we checked the Variance Inflation Factor (VIF) for each predictor. High VIF values indicate multi-collinearity, which can affect the stability and reliability of the model coefficients.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-harris-vif
#| tbl-cap: Harris MLR model Variance Inflation Factor (VIF) 

# Check VIF for Kamala Harris model
kable(vif(mlr_harris_model), digits=3)

mlr_harris_model_refined <- readRDS(here::here(("models/mlr_harris_model_refined.rds")))
```

Based on the VIF results in @tbl-harris-vif, we refined the model by removing less significant predictors (such as methodology and transparency_score) to reduce multi-collinearity.This improves the model's accuracy and interpretability.


```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-harris-refined-vif
#| tbl-cap: Harris Refined MLR model Variance Inflation Factor (VIF) 

# Refine the model by removing less significant predictors 
# (e.g., methodology and transparency_score)

# Check VIF for the refined model
kable(vif(mlr_harris_model_refined), digits=3)

step_model <- step(mlr_harris_model_refined, direction = "both", trace=0)
```

## Stepwise Model Selection


To refine the multi-linear regression model, I applied a stepwise selection method to optimize the choice of predictor variables. Stepwise selection evaluates the model iteratively, adding or removing variables based on predefined criteria (typically AIC) to find a more parsimonious model. Using both forward and backward selection, stepwise method in the step() function assesses each predictor's contribution and determines if adding or removing it improves model fit. By allowing the function to examine variables in both directions, this process systematically removed non-significant variables that did not contribute meaningfully to the model's explanatory power, leaving only the most impactful predictors in the final refined model. The resulting plots, showing the "Residuals vs Fitted" and "Normal Q-Q" diagnostic views, allow for visual inspection of model assumptions and residual patterns, supporting the quality of the refined model.

## National-Level Model Evaluation and Predictive Accuracy

After finalizing the model selection process, we evaluated the predictive performance of our models for Kamala Harris and Donald Trump using national-level polling data. Focusing on national polls, we split the dataset into training and test sets, maintaining consistency with a fixed random seed to ensure reproducibility. For each candidate, we developed a multiple linear regression model using pollscore and a log-transformed sample_size as predictors, capturing factors pertinent to national-level polling dynamics.

We then trained the models on the training subset and generated predictions for the test subset, evaluating model accuracy using the Root Mean Squared Error (RMSE). The RMSE metric quantifies the average prediction error in the test set, offering an indication of each model’s reliability in predicting future national poll outcomes. The Harris model yielded an RMSE of 3.12, while the Trump model’s RMSE was 2.40, indicating that both models provide reasonably accurate predictions, with the Trump model demonstrating slightly lower average error in predicting national polling support. These results reflect the models’ robustness and highlight their utility for assessing national-level candidate support.


```{r}
#| echo: false
#| message: false
#| warning: false
# Load necessary libraries

# Load final models for Harris and Trump
mlr_harris_model_final <- readRDS(here::here("models/mlr_harris_model_final.rds"))
mlr_trump_model_final <- readRDS(here::here("models/mlr_trump_model_final.rds"))

# Predict poll percentages for Kamala Harris and Donald Trump
harris_data$predicted_pct_harris <- predict(mlr_harris_model_final, newdata = harris_data)
trump_data$predicted_pct_trump <- predict(mlr_trump_model_final, newdata = trump_data)

```

```{r}
#| echo: false
#| message: false
#| warning: false
# Define electoral votes for each state (and Washington, D.C.)
# Correct the electoral_votes data frame
electoral_votes <- data.frame(
  state = c(
    "Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware", 
    "District of Columbia", "Florida", "Georgia", "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", 
    "Kansas", "Kentucky", "Louisiana", 
    # Maine entries
    "Maine At-Large", "Maine CD-1", "Maine CD-2", 
    "Maryland", "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", 
    # Nebraska entries
    "Nebraska At-Large", "Nebraska CD-1", "Nebraska CD-2", "Nebraska CD-3", 
    "Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota", 
    "Ohio", "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina", "South Dakota", 
    "Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington", "West Virginia", "Wisconsin", 
    "Wyoming"
  ),
  electoral_votes = c(
    9, 3, 11, 6, 55, 9, 7, 3, 3, 29, 16, 4, 4, 20, 11, 6, 6, 8, 8,
    # Maine electoral votes
    2, 1, 1,
    10, 11, 16, 10, 6, 10, 3,
    # Nebraska electoral votes
    2, 1, 1, 1,
    6, 4, 14, 5, 29, 15, 3,
    18, 7, 7, 20, 4, 9, 3,
    11, 38, 6, 3, 13, 12, 5, 10,
    3
  )
)


# Aggregate predictions by state for Harris and Trump
harris_state_avg <- aggregate(predicted_pct_harris ~ state, data = harris_data, FUN = mean)
trump_state_avg <- aggregate(predicted_pct_trump ~ state, data = trump_data, FUN = mean)

# Merge predictions for both candidates
prediction_comparison <- merge(harris_state_avg, trump_state_avg, by = "state", all.x = TRUE)

```
```{r}
#| echo: false
#| message: false
#| warning: false
# Identify any states missing from the prediction data
missing_states <- setdiff(electoral_votes$state, prediction_comparison$state)

# Assign 50% support to both candidates for missing states
missing_states_data <- data.frame(
  state = missing_states,
  predicted_pct_harris = 50,
  predicted_pct_trump = 50
)

# Append missing states data to prediction_comparison
prediction_comparison <- rbind(prediction_comparison, missing_states_data)

# Merge electoral votes into the prediction_comparison to ensure each state has electoral votes
prediction_comparison <- merge(prediction_comparison, electoral_votes, by = "state", all.x = TRUE)

```
```{r}
#| echo: false
#| message: false
#| warning: false
# Determine the winner for each state based on predicted percentages
set.seed(304)
prediction_comparison$winner <- ifelse(
  prediction_comparison$predicted_pct_harris > prediction_comparison$predicted_pct_trump, 
  "Harris", 
  ifelse(prediction_comparison$predicted_pct_trump > prediction_comparison$predicted_pct_harris, 
         "Trump", 
         sample(c("Harris", "Trump"), nrow(prediction_comparison), replace = TRUE))
)

# Calculate total electoral votes for each candidate
harris_electoral_votes <- sum(prediction_comparison$electoral_votes[prediction_comparison$winner == "Harris"], na.rm = TRUE)
trump_electoral_votes <- sum(prediction_comparison$electoral_votes[prediction_comparison$winner == "Trump"], na.rm = TRUE)

# Create a table with electoral votes
electoral_votes_table <- data.frame(
  Candidate = c("Harris", "Trump"),
  Electoral_Votes = c(harris_electoral_votes, trump_electoral_votes)
)

# Display the table with kable
kable(electoral_votes_table, caption = "Electoral Votes for Each Candidate", col.names = c("Candidate", "Electoral Votes"))
```

# Electoral College Prediction {#sec-predict}

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-predicted-vote-pct
#| fig.height: 5
#| fig.cap: "Predicted Vote Percentages by State"

# Melt data for easier plotting
prediction_melted <- melt(
  prediction_comparison, 
  id.vars = c("state", "electoral_votes", "winner"),
  measure.vars = c("predicted_pct_harris", "predicted_pct_trump")
)

# Rename melted variables for readability
colnames(prediction_melted)[colnames(prediction_melted) == "variable"] <- "candidate"
prediction_melted$candidate <- ifelse(
  prediction_melted$candidate == "predicted_pct_harris", 
  "Harris", 
  "Trump"
)

# Calculate the midpoint to split the states
n_states <- length(unique(prediction_melted$state))
midpoint <- ceiling(n_states / 2)

# Order states by electoral votes in descending order for better visualization
states_order <- prediction_melted %>%
  distinct(state, electoral_votes) %>%
  arrange(desc(electoral_votes)) %>%
  pull(state)

# Split the states into two groups
left_states <- states_order[1:midpoint]
right_states <- states_order[(midpoint + 1):length(states_order)]

# Create two separate datasets
prediction_left <- prediction_melted %>%
  filter(state %in% left_states) %>%
  mutate(state = factor(state, levels = left_states))  # Ensure consistent ordering

prediction_right <- prediction_melted %>%
  filter(state %in% right_states) %>%
  mutate(state = factor(state, levels = right_states))  # Ensure consistent ordering

# Function to create individual plots to avoid repetition
create_plot <- function(data, show_legend = FALSE) {
  ggplot(data, aes(x = reorder(state, electoral_votes), y = value, fill = candidate)) +
    geom_bar(
      stat = "identity", 
      position = position_dodge(width = 0.9), 
      width = 0.5  # Reduced bar width for more spacing
    ) +
    coord_flip() +
    labs(
      x = NULL,
      y = "Predicted Vote Percentage (%)"
    ) +
    scale_fill_manual(values = c("Harris" = "#87CEFA", "Trump" = "#F08080")) +
    theme_minimal() +
    theme(
      text = element_text(size = 9),  # Reduced overall text size
      axis.text.y = element_text(size = 8, margin = margin(r = 5)),  # Smaller state labels
      axis.text.x = element_text(size = 9),
      legend.title = element_blank(),
      legend.position = ifelse(show_legend, "top", "none"),  # Place legend only for the first plot
      legend.text = element_text(size = 9),
      panel.grid.major.y = element_blank(),
      plot.margin = margin(l = 10, r = 15, t = 10, b = 10)  # Adjusted margins
    ) +
    geom_text(
      aes(label = round(value, 1)),
      position = position_dodge(width = 0.9),
      hjust = -0.2,    # Adjusted horizontal position for labels
      size = 1.5,      # Smaller label size
      show.legend = FALSE
    ) +
    scale_y_continuous(
      expand = expansion(mult = c(0, 0.2)),  # Increased expansion for labels
      limits = c(0, 75)  # Adjust based on your data
    )
}

# Create the left plot with legend
plot_left <- create_plot(prediction_left, show_legend = TRUE)

# Create the right plot without legend
plot_right <- create_plot(prediction_right, show_legend = FALSE)

# Combine the plots side by side with legend on top
combined_plot <- plot_left + plot_right +
  plot_layout(guides = "collect") &  # Collects the legend at the top
  theme(legend.position = "top")  # Places the combined legend at the top

# Print the combined plot
combined_plot

```


To project the outcome of the 2024 U.S. Presidential election between Kamala Harris and Donald Trump, we conducted a statistical analysis that integrates predicted state-level polling percentages with the allocation of electoral votes. This methodology employs multiple linear regression models to estimate each candidate's support and translates these predictions into potential Electoral College results, adhering to the structure of the U.S. electoral system.

1. **Prediction of State-Level Polling Percentages**: We utilized finalized multiple linear regression models for both Kamala Harris and Donald Trump. These models were applied to their respective datasets to generate predicted polling percentages for each candidate across various states. The predictions reflect the expected levels of support based on historical polling data and relevant covariates included in the models.

2. **Aggregation of Predicted Support by State**: For each candidate, we computed the average predicted polling percentage within each state. This aggregation provides a concise state-level summary of anticipated support, facilitating direct comparisons between the candidates in each state (see @fig-predicted-vote-pct).

3. **Integration of Electoral Vote Allocations**: We established a dataset detailing the electoral vote distribution for each state, including the distinct allocations for Maine and Nebraska, which can split their electoral votes by congressional district. This dataset was merged with the state-level predicted support data to align electoral votes with the corresponding predicted support.

4. **Addressing Missing Data**: We identified states lacking predicted polling data and assigned a neutral predicted support of 50% to both candidates in these states. This assumption accounts for the absence of data while ensuring that all states are included in the analysis.

5. **Determination of State Winners**: Based on the aggregated predicted percentages, we determined the projected winner in each state. A candidate was designated as the winner if their predicted polling percentage exceeded that of their opponent. In cases where the predicted percentages were equal, we randomly assigned the winner using a stochastic approach to reflect the uncertainty inherent in tied predictions.

6. **Calculation of Total Electoral Votes**: We summed the electoral votes from all states won by each candidate to compute their total electoral vote counts. This calculation is critical, as securing at least 270 electoral votes is necessary to win the presidency under the U.S. electoral system.

7. **Interpretation of Results**: The analysis indicated that Kamala Harris is projected to receive a total of **306 electoral votes**, while Donald Trump is projected to receive **232 electoral votes**. Harris surpasses the 270-vote threshold required for victory, suggesting a strong position in the Electoral College based on our predictions. This outcome implies that Harris is likely to win the election, given her substantial lead in projected electoral votes (see @fig-map-state-winner).

This statistical approach effectively connects state-level polling data to Electoral College projections. By employing multiple linear regression models to predict polling percentages and integrating these predictions with the electoral vote framework, we provide a detailed analysis of potential election results. This methodology reflects the structure of the U.S. electoral system and offers valuable insights into how predicted voter support may translate into electoral success.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-map-state-winner
#| fig.cap: "Electoral College Predition Map"

# Get US state map data
us_states <- map_data("state")

# Create a mapping of state names
state_mapping <- data.frame(
  state = c(
    "Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware",
    "District of Columbia", "Florida", "Georgia", "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa",
    "Kansas", "Kentucky", "Louisiana", "Maine At-Large", "Maine CD-1", "Maine CD-2", "Maryland",
    "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", "Montana",
    "Nebraska At-Large", "Nebraska CD-1", "Nebraska CD-2", "Nebraska CD-3",
    "Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York", "North Carolina",
    "North Dakota", "Ohio", "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island",
    "South Carolina", "South Dakota", "Tennessee", "Texas", "Utah", "Vermont",
    "Virginia", "Washington", "West Virginia", "Wisconsin", "Wyoming"
  ),
  region = tolower(c(
    "alabama", "alaska", "arizona", "arkansas", "california", "colorado", "connecticut", "delaware",
    "district of columbia", "florida", "georgia", "hawaii", "idaho", "illinois", "indiana", "iowa",
    "kansas", "kentucky", "louisiana", "maine", "maine", "maine", "maryland",
    "massachusetts", "michigan", "minnesota", "mississippi", "missouri", "montana",
    "nebraska", "nebraska", "nebraska", "nebraska",
    "nevada", "new hampshire", "new jersey", "new mexico", "new york", "north carolina",
    "north dakota", "ohio", "oklahoma", "oregon", "pennsylvania", "rhode island",
    "south carolina", "south dakota", "tennessee", "texas", "utah", "vermont",
    "virginia", "washington", "west virginia", "wisconsin", "wyoming"
  ))
)

# Simplify prediction data for mapping
map_data <- prediction_comparison %>%
  select(state, winner) %>%
  left_join(state_mapping, by = "state")

# Merge with map data
map_data_final <- us_states %>%
  left_join(map_data, by = "region")

# Create the basic map without projection
ggplot(data = map_data_final) +
  geom_polygon(aes(x = long, y = lat, group = group, fill = winner), 
               color = "white", size = 0.2) +
  coord_fixed(1.3) +  # This replaces coord_map() with a simpler aspect ratio adjustment
  scale_fill_manual(values = c("Harris" = "#87CEFA", "Trump" = "#F08080"),
                    name = "Predicted Winner") +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 16),
    legend.position = "bottom"
  )
```

# Discussion {#sec-discuss}

## Summary of Findings

In this paper, we developed statistical models to forecast the outcome of the 2024 United States presidential election between Kamala Harris and Donald Trump. Utilizing polling data from FiveThirtyEight's 2024 Presidential Election Forecast Database, we analyzed factors influencing candidate support through linear and multiple linear regression models. We incorporated variables such as pollster rating, transparency score, sample size, and state-level data to predict the percentage of support for each candidate across different states. By aggregating these state-level predictions and integrating electoral vote allocations, we projected that Kamala Harris would receive 306 electoral votes, surpassing the 270 required to win the presidency.

## Insights on Polling Methodologies and Voter Support

Our analysis indicates that pollster rating and state-level factors significantly influence reported support for both candidates. For Kamala Harris, the multiple linear regression model shows that higher pollster ratings and certain states are associated with increased support percentages. This suggests that polls conducted by more reliable pollsters may report higher support for Harris, possibly due to methodological rigor or sampling techniques that capture her voter base more accurately. For Donald Trump, pollster transparency and state differences play a notable role, indicating that how openly pollsters disclose their methodologies can affect the reported support for Trump.

## Implications for Electoral Forecasting

The projection of Kamala Harris receiving 306 electoral votes underscores the importance of incorporating state-level analyses in electoral forecasting. Our findings highlight that national polling averages may not sufficiently capture the complexities of the Electoral College system. By focusing on state-specific predictions and accounting for variables that influence voter support in different regions, forecasters can achieve more accurate predictions. This approach acknowledges the heterogeneity of the electorate and the pivotal role of battleground states in determining election outcomes.

## Limitations

Despite the strengths of our modeling approach, several limitations must be acknowledged. First, the reliance on historical polling data and pollster ratings assumes that past performance is indicative of future accuracy, which may not hold true if polling methodologies or voter behaviors change significantly. Second, the underrepresentation of certain states due to sparse polling data may affect the reliability of our state-level predictions. Assigning a default 50% support in states with missing data introduces uncertainty and may not reflect actual voter preferences. Additionally, the models may not fully account for dynamic factors such as late-breaking events, shifts in voter sentiment, or turnout variations that can influence election results.

## Future Research and Recommendations

To enhance the accuracy of electoral forecasts, future research should consider incorporating real-time data and alternative data sources such as social media trends, economic indicators, and demographic shifts. Employing more sophisticated modeling techniques, such as hierarchical Bayesian models or machine learning algorithms, may capture complex interactions and non-linear relationships among variables. Furthermore, improving polling methodologies to address issues of response bias and sample representativeness is essential. Collaborations between pollsters, statisticians, and political scientists can foster the development of more robust predictive models that adapt to the evolving electoral landscape.


\newpage

\appendix

# Appendix {#sec-appendix}

## YouGov Pollster Methodology Overview and Evaluation
YouGov conducts online surveys through their proprietary panel of U.S. adults, using nonprobability sampling methods combined with sophisticated weighting procedures to achieve representative results. Their approach balances speed and cost-effectiveness with statistical rigor through careful sample selection and data quality controls.

### Survey Population and Sampling
YouGov's target population typically comprises all U.S. adults or adult citizens, with their sampling frame consisting of their opt-in online panel covering approximately 95% of Americans. For general population surveys, they aim for 1,000-2,000 respondents, selected based on demographic and political characteristics to match the target population.

### Panel Recruitment and Participation
Panel members are recruited through advertising and website partnerships, with surveys offered in multiple languages including Spanish to ensure broad representation. Participants receive points exchangeable for small monetary rewards, though many report being motivated by the desire to contribute to research.

### Quality Control
YouGov employs several measures to maintain data quality:

- Verification of panelist identity through email and IP checks
- Response quality surveys to gauge reliability
- Monitoring of response times and patterns
- Removal of respondents who fail quality checks
- Question randomization to reduce bias

### Non-response and Weighting
To address potential biases, YouGov applies statistical weighting based on demographics (age, gender, race, education) and political factors (voting behavior, party identification). Their weighting process considers multiple characteristics simultaneously to better reflect real-world demographic intersections.

### Strengths and Limitations
The methodology's primary strengths include rapid data collection, cost-effectiveness, and the ability to track opinions over time. However, the nonprobability sampling approach may introduce biases, and the online-only format could underrepresent certain populations. While weighting helps address these limitations, it cannot fully account for all potential sources of bias.

\newpage
## Idealized Survey Methodology
This idealized survey methodology outlines a comprehensive plan for forecasting the US presidential election within a budget of $100,000. The approach is designed to be statistically sound, practical, and capable of accurately predicting election outcomes by considering both the popular vote and electoral college implications.

### Sampling Strategy
The target population for this survey is eligible voters across the United States who are likely to participate in the upcoming presidential election. To achieve a representative sample:

- **Sampling Frame:** Utilize a combination of registered voter lists and demographic data from reputable sources such as the US Census Bureau.
- **Sampling Method:** Implement stratified random sampling to ensure representation across key demographics, including age, gender, race, education level, and geographic location.
- **Sample Size Calculation:** Aim for a sample size of approximately 10,000 respondents to achieve a margin of error of ±1% at a 95% confidence level.
- **Geographical Distribution:** Allocate samples proportionally across all 50 states and the District of Columbia, with oversampling in swing states to better predict electoral college outcomes.
- **Addressing Sampling Biases:** Apply weighting adjustments to account for underrepresented groups and ensure that the sample mirrors the overall voter population.

### Recruitment Plan
To recruit respondents effectively, we will leverage online panels, social media advertising, and partnerships with community organizations to reach a diverse audience. Offering modest incentives, such as $5 digital gift cards, encourages participation while managing costs. Quota sampling within strata maintains demographic balance, and follow-up reminders along with mobile-friendly survey formats help reduce non-response bias. The data collection will occur over a two-week period to capture timely opinions without introducing temporal biases.

### Survey Design Elements
The survey is crafted to elicit accurate and meaningful responses:

- **Question Types and Formats:** Use a mix of closed-ended questions and multiple-choice options for clarity and ease of analysis.
- **Response Options:** Include balanced and neutral response choices, with options for "Undecided" or "Prefer not to say."
- **Question Order and Flow:** Begin with general questions to build rapport, followed by more specific vote intention queries, and conclude with demographic questions.
- **Demographic Information:** Collect data on age, gender, race, education, income, and geographic location.
- **Political Affiliation and History:** Ask about party affiliation, past voting behavior, and political engagement.
- **Likely Voter Screens:** Include questions to gauge voting likelihood, such as past voting frequency and intention to vote in the upcoming election.
- **Vote Intention Questions:** Directly ask which candidate the respondent intends to vote for, ensuring confidentiality and anonymity.

### Quality Control
To maintain data integrity, we implement several quality control measures. Real-time validation checks within the survey prevent inconsistent or illogical responses. Attention-check questions identify disengaged respondents. We use unique survey links and track IP addresses to prevent duplicate submissions, while CAPTCHA verification deters automated responses. Incomplete or suspicious responses are excluded during data cleaning to ensure the final dataset is robust and reliable.

### Data Processing
These data processing steps will be taken to ensure accurate analysis:

- **Weighting Methodology:** Adjust survey results using weighting factors based on demographic proportions in the voting population.
- **Handling Missing Data:** Employ imputation techniques or exclude cases with significant missing information.
- **Outlier Detection:** Identify and review outliers that may skew results, determining whether to retain or discard them.
- **Response Validation:** Cross-check responses for consistency and plausibility.
- **Poll Aggregation Approach:** Combine survey data with other reputable polls using meta-analytic techniques to enhance prediction accuracy.

### Budget Allocation
A budget allocation of $100,000 ensures all aspects are adequately funded:

- **Recruitment Costs:** $40,000 for advertising and partnerships to reach potential respondents.
- **Incentive Payments:** \$50,000 allocated for participant incentives ($5 x 10,000 respondents).
- **Survey Platform Fees:** $2,000 for premium features on a survey platform like Google Forms or an equivalent.
- **Data Analysis Tools:** $3,000 for statistical software licenses and data processing tools.
- **Quality Control Measures:** $3,000 for implementing validation systems and CAPTCHA services.
- **Administrative Costs:** $2,000 for project management and miscellaneous expenses.

### Conclusion
This methodology presents a feasible and thorough plan to forecast the US presidential election within the specified budget. By adhering to best practices in survey design and execution, and by carefully considering both the popular vote and electoral college implications, the survey aims to provide accurate and reliable insights into voter intentions.


\newpage

# Appendix 

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-overview_data
#| tbl-cap: Sample Overview of Selected Variables in the Polling Dataset

set.seed(778) # For reproducability

data_overview <- data %>%
  select(pollster, pollscore, numeric_grade, transparency_score, sample_size, 
         methodology, pct, state, candidate_name, end_date)

# Transpose the first 5 rows to display them vertically
data_overview <- data_overview %>%
  sample_n(5) %>%
  t()

# Display the transposed table
kable(data_overview)
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-poll-metrics
#| tbl-cap: Summary Statistics of Key Poll Metrics

data %>%
  summarise(
    across(c(sample_size, pollscore, numeric_grade, transparency_score),
           list(
             Mean = mean,
             Median = median,
             SD = sd,
             Min = min,
             Max = max
           ), 
           .names = "{.fn}_{.col}",
           na.rm = TRUE)
  ) %>%
  pivot_longer(everything(), 
               names_to = c("Statistic", "Variable"),
               names_sep = "_",
               values_to = "Value") %>%
  pivot_wider(names_from = Variable,
              values_from = Value) %>%
  mutate(across(-Statistic, ~round(.x, 2))) %>%
  kable()
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-methodology-dist
#| fig-cap: Distribution of Polling Methodologies

data %>%
  ggplot(aes(x = fct_infreq(methodology))) +
  geom_bar() +
  coord_flip() +
  theme_minimal() +
  labs(x = "Methodology",
       y = "Count") +
  theme(axis.text.y = element_text(size = 8))
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-candidate-support
#| fig-cap: Distribution of Support Percentages by Candidate

ggplot(data, aes(x = pct, fill = candidate_name)) +
  geom_histogram(bins = 30, position = "identity") +
  facet_wrap(~ candidate_name) +
  labs(x = "Percentage of Support",
       y = "Count") +
  scale_fill_discrete(name = "Candidates") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-state-polls
#| tbl-cap: Polling Frequency by State

data %>%
  count(state) %>%
  arrange(desc(n)) %>%
  rename("State" = state,
         "Number of Polls" = n) %>%
  kable(format = "latex", booktabs = TRUE)
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-temporal-polling
#| fig-cap: Number of Polls Over Time

data %>%
  mutate(end_date = as.Date(end_date)) %>%
  mutate(week = floor_date(end_date, "week")) %>%
  count(week) %>%
  ggplot(aes(x = week, y = n)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(x = "Week",
       y = "Number of Polls") +
  scale_x_date(date_breaks = "2 weeks", date_labels = "%b %d")
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-sample-size
#| fig-cap: Distribution of Poll Sample Sizes

data %>%
  ggplot(aes(x = sample_size)) +
  geom_histogram(binwidth = 100) +
  theme_minimal() +
  labs(x = "Sample Size",
       y = "Count") +
  scale_x_continuous(breaks = seq(0, max(data$sample_size), by = 500))
```


```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-harris-samplesize-supportpct
#| fig-cap: Linear Regression of Percentage vs Sample Size for Kamala Harris

# Plot the relationship for Kamala Harris
ggplot(harris_data, aes(x = sample_size, y = pct)) +
  geom_point() +
  geom_smooth(method = "lm", col = "blue") +
  labs(x = "Sample Size",
       y = "Percentage")
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-trump-samplesize-supportpct
#| fig-cap: Linear Regression of Percentage vs Sample Size for Donald Trump

# Plot the relationship for Trump
ggplot(trump_data, aes(x = sample_size, y = pct)) +
  geom_point() +
  geom_smooth(method = "lm", col = "blue") +
  labs(x = "Sample Size",
       y = "Percentage")
```
```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-mlr-harris
#| fig-cap: Multi-Linear Regression model for Kamala Harris
#| fig.width: 10
#| fig.height: 4

mlr_harris_model <- readRDS(here::here(("models/mlr_harris_model.rds")))

# For Harris model
par(mfrow = c(1, 2))  # Set layout for 1 row and 2 columns
plot(mlr_harris_model, which = c(1, 2))  # 1: Residuals vs Fitted, 2: Normal Q-Q
par(mfrow = c(1, 1))  # Reset plot layout

```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-mlr-trump
#| fig-cap: Multi-Linear Regression model for Donald Trump
#| fig.width: 10
#| fig.height: 4

# Load the saved MLR model for Donald Trump
mlr_trump_model <- readRDS(here::here(("models/mlr_trump_model.rds")))

# For Trump model
par(mfrow = c(1, 2))  # Set layout for 1 row and 2 columns
plot(mlr_trump_model, which = c(1, 2))  # 1: Residuals vs Fitted, 2: Normal Q-Q
par(mfrow = c(1, 1))  # Reset plot layout

```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-harris-vif
#| fig-cap: Harris Refined Multi-Linear Regression Model
#| fig.width: 10
#| fig.height: 4

# Refine the model by removing less significant predictors 
# (e.g., methodology and transparency_score)
mlr_harris_model_refined <- readRDS(here::here(("models/mlr_harris_model_refined.rds")))

# For Harris refined model
par(mfrow = c(1, 2))  # Set layout for 1 row and 2 columns
plot(mlr_harris_model_refined, which = c(1, 2))  # 1: Residuals vs Fitted, 2: Normal Q-Q
par(mfrow = c(1, 1))  # Reset plot layout

```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-harris-step
#| fig-cap: Harris Final Multi-Linear Regression Model
#| fig.width: 10
#| fig.height: 4

# Perform stepwise selection to optimize the Harris model
step_model <- step(mlr_harris_model_refined, direction = "both", trace=0)

# For Harris stepwise model
par(mfrow = c(1, 2))  # Set layout for 1 row and 2 columns
plot(step_model, which = c(1, 2))  # 1: Residuals vs Fitted, 2: Normal Q-Q
par(mfrow = c(1, 1))  # Reset plot layout

```

\newpage

# References